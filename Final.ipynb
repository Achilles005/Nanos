{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aditya.shukla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aditya.shukla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Required Libraries\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from itertools import product\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec, KeyedVectors   \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models import word2vec\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import OrderedDict\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import datetime\n",
    "import gensim\n",
    "import math\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stops = set(stopwords.words(\"english\"))\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to extract contents from the mentioned URL by the User\n",
    "class Content_Extract:\n",
    "    def __init__(self, url):\n",
    "        self.url=url\n",
    "    \n",
    "    def content_extract(self,url):\n",
    "        def condense_newline(text):\n",
    "            return '\\n'.join([p for p in re.split('\\n|\\r', text) if len(p) > 0])\n",
    "        page = urlopen(self.url)\n",
    "        html = page.read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        TAGS = ['p','h1','h2','h3','h4','h5','h6','h7']\n",
    "        content=' '.join([condense_newline(tag.text) for tag in soup.findAll(TAGS)])\n",
    "        \n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def preprocess_spacy(text):\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    text=re.sub(\"[^a-zA-Z.]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "    return frozenset(stop_set)\n",
    "\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    #remove tags\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    text=text.split()\n",
    "    stopwords=get_stop_words(\"stopwords.txt\")\n",
    "    #text = [stemmer.stem(word) for word in text]\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text=[w for w in text if not w in stopwords]\n",
    "    text= \" \".join( text )\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"[^a-zA-Z.]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def extract():\n",
    "    url=input('Please mention the url : ')\n",
    "    content=Content_Extract(url)\n",
    "    content=content.content_extract(content)\n",
    "    return content\n",
    "\n",
    "def initital_words_extract():\n",
    "    product=input('Enter product: ')\n",
    "    service=input('Enter Service: ')\n",
    "    product=product.lower()\n",
    "    service=service.lower()\n",
    "    initial_words=product.split(\" \")\n",
    "    for i in service.split():\n",
    "        if i not in initial_words:\n",
    "            initial_words.append(i)\n",
    "    return initial_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please mention the url : https://www.nanos.ai/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nanos is a do-it-yourself\\npay-as-you-go\\nadvertising tool  Create optimized ad campaigns on Google, Facebook, and Instagram all in one place with Nanos patented Artificial Intelligence technology  *No credit card required Become your own marketing agency in 4 easy steps No website, no Facebook Business page required Online marketing should be this simple Itâ€™s like having an agency in your pocket! Your browser cannot play the provided video file. Step 1: Sign up at Nanos and tell us in a few words about your great product or service Your browser cannot play the provided video file. Step 2: Nanos AI will suggest ad text, keywords, interests, and platforms for your ad Your browser cannot play the provided video file. Step 3: Decide on the budget you want to spend, and how long to run your ads Your browser cannot play the provided video file. Step 4: Log in to your dashboard and watch Nanos AI in action optimizing at lightning speed As seen on: *No credit card required The digital marketing superhero for every kind of business Whether you have a physical store or an online venture, the goal is the same: to grow your business! We understand that existing marketing tools may be too complicated and expensive. After all, not everyone is a marketing expert or can afford to hire one. No website or landing page? Weâ€™ve got you covered! Your browser cannot play the provided video file. Features Donâ€™t want to miss out on any new features? Sign up for our newsletter. Cross-channel advertising Draft a campaign with multiple ads and publish them on several platforms simultaneously, from one single dashboard. Nanos currently enables you to create campaigns for Google, Facebook, and Instagram. Ad and campaign types Single images, videos, or carousel ads. Pick the option that best fits your ad or test different ones. Nanos supports traffic campaigns for all of our supported platforms and conversion campaigns for Facebook. Ad placement Choose where you want to place your ad. On Facebook, choose between News Feed, Marketplace, Stories, and Search Results. On Instagram, your options are Feed, Stories, and Explore. If you want, you can let our technology choose the best placement for your ad. Budget optimization Nanosâ€™ machine learning technology optimizes your budget by reshuffling it daily and discarding the lowest performing platforms, keywords, and interests. Keyword, bid, and interest optimization Our AI-powered technology chooses the best keywords, bids, and interests for your campaign according to your specific campaign data and updates them intelligently so that you get the best results possible. AI-powered text suggestions Our technology suggests headlines and descriptions for your ad, based on the information about your business. You can use them as they are, change them, or replace them completely. Ready to start your campaign? *No credit card required Success stories At Nanos, we help make success stories one business at a time.  Nanos for Agencies Are you running a small to midsize agency?\\nLooking for a tool to optimize your clientsâ€™ budgets and your own costs on managing the overhead expenses for ad campaign design, text, placement and cross-platform, keywords and interests optimization, and reporting? Thanks to Nanocorp AG, I can create my online marketing campaign with a single login and use the effective evaluation to control and adapt the online marketing strategy of my company constantly. Thanks to the great support and experience of the team, I receive useful tips in terms of online marketing and business success. Nanos makes it possible: Finally, we can incorporate online advertising into the communication mix for our customers with small budgets. The results are more than convincing and the numbers are getting better everyday thanks to Nanos. Working with the entire Nanos team is very cooperative and efficient. I struggled a lot with digital advertising, hired lots of freelancers, but I could never find anyone I could really trust that would generate high performances. With Nanos, I can create campaigns for my clients, knowing that Nanos technology will do the best for me â€¦ And I love that theyâ€™re always available via chat! As a startup, I wanted to have a tool which would allow me to  build my brand Happy Pijama and to grow sales in a simple and clear way, without spending lots of time and people resources. During Web Summit 2019 in Lisbon, I came across Nanos where they presented their tool and I thought, â€œThis is exactly what I need,â€ so I placed my campaigns.  Placing online ads on Nanos is a really smooth process. All in all, I had a really great experience for the post-ad period. The reports for each category are pretty detailed. We at Physio & Co are very grateful to be introduced to Nanos. Nanos took the time to understand our business and our needs, and was able to offer simple, affordable solutions. Our patients can now find us easily and thanks to Nanos, we now have an efficient, effective marketing channel, with almost zero effort from our side. Thank you Nanos ðŸ™‚ From a tech perspective, Nanos is an up-and-coming marketing technology innovation thatâ€™s literally saved us time when it comes to placing online ads. The interface is much more pleasant to work with and the ML tech that runs it is unprecedented. Iâ€™ve used it for my business and I will use it again.\\nread more What I like about working with Nanos is that there is one interface that allows setting things up on a lot of different platforms. I like the ease of use of the system â€“ itâ€™s not too complex, you can figure it out fairly quickly. Your browser cannot play the provided video file. A Win-Win Solution At Nanos, we take the confusion out of online advertising. Nanos fees are transparent: we charge 17% of your total advertising budget, and you can pause or cancel your ad campaign any time. Any remaining funds get refunded to your account automatically! Nanos has a proven track record for bringing in new clients for business like yours: restaurants, auto sales, real estate agencies, video game developers, online courses, and wedding planners. *No credit card required Stay tuned! Sign up for our newsletter to make sure you donâ€™t miss a thing  From our blog SEO for Beginners: 10 Steps to Optimizing Your Website Now How Does AI Marketing Give You Added Leverage? How To Create An Effective Ad For Your Video Production Company 7 Tips for E-Commerce Personalization We are currently seeking international partnerships Become a partner and ambassador of Nanos to be part of a network of strategic collaborations that give businesses big and small access to marketing power they need! Privacy Overview Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information. Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website. About Us\\nFAQ\\nContact Videos\\nAI Engine\\nCase Studies\\nBlog\\nBuild a Free Landing Page\\nBook a Free Consultation Terms of Use\\nPrivacy Policy Nanos Â© 2021'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting Content\n",
    "content=extract()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding: \n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        self.verbose = verbose\n",
    "        self.model = {}\n",
    "        \n",
    "    def convert(self, source, ipnut_file_path, output_file_path):\n",
    "        if source == 'glove':\n",
    "            input_file = datapath(ipnut_file_path)\n",
    "            output_file = get_tmpfile(output_file_path)\n",
    "            glove2word2vec(input_file, output_file)\n",
    "        elif source == 'word2vec':\n",
    "            pass\n",
    "        elif source == 'fasttext':\n",
    "            pass\n",
    "        elif source == 'homemade_embedding':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext, or HomeMadeEmbedding')\n",
    "        \n",
    "    def load(self, source, file_path):\n",
    "        print(datetime.datetime.now(), 'start: loading', source)\n",
    "        if source == 'glove':\n",
    "            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        elif source == 'word2vec':\n",
    "            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "        elif source == 'fasttext':\n",
    "            self.model[source] = gensim.models.wrappers.FastText.load_fasttext_format(file_path)\n",
    "        elif source == 'homemade_embedding':\n",
    "            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "        else:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext, or HomeMadeEmbedding')\n",
    "            \n",
    "        print(datetime.datetime.now(), 'end: loading', source)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def get_model(self, source):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext', 'homemade_embedding']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext, or HomeMadeEmbedding')\n",
    "            \n",
    "        return self.model[source]\n",
    "    \n",
    "    def get_words(self, source, size=None):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext', 'homemade_embedding']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext, or HomeMadeEmbedding')\n",
    "        \n",
    "        if source in ['glove', 'word2vec','homemade_embedding']:\n",
    "            if size is None:\n",
    "                return [w for w in self.get_model(source=source).vocab]\n",
    "            elif size is None:\n",
    "                return [w for w in self.get_model(source=source).vocab]\n",
    "            else:\n",
    "                results = []\n",
    "                for i, word in enumerate(self.get_model(source=source).vocab):\n",
    "                    if i >= size:\n",
    "                        break\n",
    "                        \n",
    "                    results.append(word)\n",
    "                return results\n",
    "            \n",
    "        elif source in ['fasttext']:\n",
    "            if size is None:\n",
    "                return [w for w in self.get_model(source=source).wv.vocab]\n",
    "            else:\n",
    "                results = []\n",
    "                for i, word in enumerate(self.get_model(source=source).wv.vocab):\n",
    "                    if i >= size:\n",
    "                        break\n",
    "                        \n",
    "                    results.append(word)\n",
    "                return results\n",
    "        \n",
    "        return Exception('Unexpected flow')\n",
    "    \n",
    "    def get_dimension(self, source):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext', 'homemade_embedding']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext, or HomeMadeEmbedding')\n",
    "        \n",
    "        if source in ['glove', 'word2vec','homemade_embedding']:\n",
    "            return self.get_model(source=source).vectors[0].shape[0]\n",
    "            \n",
    "        elif source in ['fasttext']:\n",
    "            word = self.get_words(source=source, size=1)[0]\n",
    "            return self.get_model(source=source).wv[word].shape[0]\n",
    "        \n",
    "        return Exception('Unexpected flow')\n",
    "    \n",
    "    def get_vectors(self, source, words=None):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext', 'homemade_embedding']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext, or HomeMadeEmbedding')\n",
    "        \n",
    "        if source in ['glove', 'word2vec', 'fasttext', 'homemade_embedding']:\n",
    "            if words is None:\n",
    "                words = self.get_words(source=source)\n",
    "            \n",
    "            embedding = np.empty((len(words), self.get_dimension(source=source)), dtype=np.float32)            \n",
    "            for i, word in enumerate(words):\n",
    "                embedding[i] = self.get_vector(source=source, word=word)\n",
    "                \n",
    "            return embedding\n",
    "        \n",
    "        return Exception('Unexpected flow')\n",
    "    \n",
    "    def get_vector(self, source, word, oov=None):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext', 'homemade_embedding']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext, or HomeMadeEmbedding')\n",
    "            \n",
    "        if source not in self.model:\n",
    "            raise ValueError('Did not load %s model yet' % source)\n",
    "        \n",
    "        try:\n",
    "            return self.model[source][word]\n",
    "        except KeyError as e:\n",
    "            raise\n",
    "            \n",
    "    def get_synonym(self, source, word, oov=None):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext', 'homemade_embedding']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext, or HomeMadeEmbedding')\n",
    "            \n",
    "        if source not in self.model:\n",
    "            raise ValueError('Did not load %s model yet' % source)\n",
    "        \n",
    "        try:\n",
    "            return self.model[source].most_similar(positive=word, topn=5)\n",
    "        except KeyError as e:\n",
    "            raise\n",
    "    \n",
    "    def which_distance_between_two_words(self, source, word1, word2, oov=None):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext', 'homemade_embedding']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext, or HomeMadeEmbedding')\n",
    "            \n",
    "        if source not in self.model:\n",
    "            raise ValueError('Did not load %s model yet' % source)\n",
    "        \n",
    "        try:\n",
    "            return self.model[source].similarity(word1, word2)\n",
    "        except KeyError as e:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class function to calculate TF-IDF Score\n",
    "class tfidf:\n",
    "    def __init__(self,content):\n",
    "        self.content=content\n",
    "        \n",
    "        \n",
    "    def tfidf_score(self,content):\n",
    "        total_words = self.content.split(\" \")\n",
    "        total_word_length = len(total_words)\n",
    "        total_sentences = tokenize.sent_tokenize(self.content)\n",
    "        total_sent_len = len(total_sentences)\n",
    "        tf_score = {}\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        for each_word in total_words:\n",
    "            each_word = each_word.replace('.','')\n",
    "            if each_word not in stops:\n",
    "                if each_word in tf_score:\n",
    "                    tf_score[each_word] += 1\n",
    "                else:\n",
    "                    tf_score[each_word] = 1\n",
    "                    \n",
    "        tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "        \n",
    "        def check_sent(word, sentences): \n",
    "            final = [all([w in x for w in word]) for x in sentences] \n",
    "            sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "            return int(len(sent_len))\n",
    "        \n",
    "        idf_score = {}\n",
    "        \n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        \n",
    "        for each_word in total_words:\n",
    "            each_word = each_word.replace('.','')\n",
    "            if each_word not in stops:\n",
    "                if each_word in idf_score:\n",
    "                    idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "                else:\n",
    "                    idf_score[each_word] = 1\n",
    "\n",
    "        idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "        \n",
    "        tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "        \n",
    "        result = dict(sorted(tf_idf_score.items(), key = itemgetter(1), reverse = True)) \n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Keyword:\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        ret=list()\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            ret.append(key)\n",
    "            if i > number:\n",
    "                break\n",
    "        return ret\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 13:49:21.577953 start: loading word2vec\n",
      "2021-03-12 13:51:19.132078 end: loading word2vec\n",
      "2021-03-12 13:51:19.186067 start: loading glove\n",
      "2021-03-12 14:07:59.452988 end: loading glove\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.WordEmbedding at 0x243da2f3d90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading word embedding files(Takes time can choose either of these 3 depending upon the use-case)\n",
    "glove_file_path = 'glove.840B.300d.vec'\n",
    "word2vec_file_path = 'GoogleNews-vectors-negative300.bin'\n",
    "#fasttext_file_path = 'wiki.en.bin'\n",
    "\n",
    "word_embedding = WordEmbedding()\n",
    "word_embedding.load(source='word2vec', file_path=word2vec_file_path)\n",
    "word_embedding.load(source='glove', file_path=glove_file_path)\n",
    "#word_embedding.load(source='fasttext', file_path=fasttext_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter product: digital marketing\n",
      "Enter Service: digital marketing tool\n"
     ]
    }
   ],
   "source": [
    "text=pre_process(content)\n",
    "initial_words=initital_words_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "related=dict()\n",
    "for i,j in product(initial_words,text.split(\" \")):\n",
    "    #print(i+\" \"+j)\n",
    "    for source in ['glove','word2vec', 'fasttext']:\n",
    "        #print('Source: %s' % (source))\n",
    "        try:\n",
    "            score=word_embedding.which_distance_between_two_words(source=source,word1=i, word2=j)\n",
    "            if j not in related.keys():\n",
    "                related.update({j:score})\n",
    "            else:\n",
    "                if related[j]< score:\n",
    "                    related.update({j:score})\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=tfidf(text)\n",
    "potential_list=t.tfidf_score(text)\n",
    "potential_list=potential_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in product(initial_words,potential_list):\n",
    "    #print(i+\" \"+j)\n",
    "    for source in ['glove','word2vec', 'fasttext']:\n",
    "        #print('Source: %s' % (source))\n",
    "        try:\n",
    "            score=word_embedding.which_distance_between_two_words(source=source,word1=i, word2=j)\n",
    "            if j not in related.keys():\n",
    "                related.update({j:score})\n",
    "            else:\n",
    "                if related[j]< score:\n",
    "                    related.update({j:score})\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using Page Rank Algo to find most relevant words\n",
    "content=preprocess_spacy(content)\n",
    "length=len(content)\n",
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(content, candidate_pos = ['NOUN'], window_size=10, lower=True)\n",
    "l=tr4w.get_keywords(length)\n",
    "stopwords=get_stop_words(\"stopwords.txt\")\n",
    "doc1 = [w for w in l if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = initial_words\n",
    "for i in w1:\n",
    "    for j in l:\n",
    "        tokens=nlp(i+str(\" \")+j)\n",
    "        token1, token2=tokens[0],tokens[1]\n",
    "        if j not in related.keys():\n",
    "                related.update({j:token1.similarity(token2)})\n",
    "        else:\n",
    "            if related[j]< score:\n",
    "                related.update({j:token1.similarity(token2)})\n",
    "        #print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advertising\n",
      "tool\n",
      "ad\n",
      "technology\n",
      "marketing\n",
      "easy\n",
      "business\n",
      "video\n",
      "product\n",
      "digital\n",
      "enables\n",
      "images\n",
      "marketplace\n",
      "optimization\n",
      "success\n",
      "clients\n",
      "strategy\n",
      "company\n",
      "customer\n",
      "brand\n",
      "web\n",
      "innovation\n",
      "sales\n",
      "seo\n",
      "strategic\n",
      "functionality\n",
      "analytics\n",
      "campaigns\n",
      "solutions\n"
     ]
    }
   ],
   "source": [
    "for i in related.keys():\n",
    "    if related[i]>0.50:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
